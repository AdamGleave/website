---
abstract: It's challenging to design reward functions for complex, real-world tasks. Reward learning lets one instead infer reward functions from data.  However, multiple reward functions often fit the data equally well, even in the infinite-data limit.  Prior work often considers reward functions to be uniquely recoverable, by imposing additional assumptions on data sources. By contrast, we formally characterise the partial identifiability of popular data sources, including demonstrations and trajectory preferences, under multiple com- mon sets of assumptions. We analyse the impact of this partial identifiability on downstream tasks such as policy optimisation, including under changes in environment dynamics. We unify our results in a framework for comparing data sources and downstream tasks by their invariances, with implications for the design and selection of data sources for reward learning. 
abstract_short: ' '
authors: [JoarSkalse, MatthewFarrugiaRoberts, StuartRussell, AlessandroAbate, AdamGleave]
date: '2022-03-13'
highlight: false
image_preview: ''
math: false
publication: International Conference on Machine Learning
publication_short: ICML
publication_types: ['1']
selected: false
title: Invariance in Policy Optimisation and Partial Identifiability in Reward Learning
url_pdf: https://arxiv.org/pdf/2203.07475.pdf
url_dataset: ''
url_project: ''
url_slides: '' 
url_video: ''
---

