---
abstract: Multi-task Inverse Reinforcement Learning (IRL) is the problem of inferring
  multiple reward functions from expert demonstrations. Prior work, built on Bayesian
  IRL, is unable to scale to complex environments due to computational constraints.
  This paper contributes a formulation of multi-task IRL in the more computationally
  efficient Maximum Causal Entropy (MCE) IRL framework. Experiments show our approach
  can perform one-shot imitation learning in a gridworld environment that single-task
  IRL algorithms need hundreds of demonstrations to solve. We outline preliminary
  work using meta-learning to extend our method to the function approximator setting
  of modern MCE IRL algorithms. Evaluating on multi-task variants of common simulated
  robotics benchmarks, we discover serious limitations of these IRL algorithms, and
  conclude with suggestions for further work
abstract_short: ' '
authors: [AdamGleave, Oliver Habryka]
date: '2018-07-15'
highlight: false
image_preview: ''
math: false
publication: Workshop on Goal Specifications for Reinforcement Learning at Federated
  Artificial Intelligence Meeting
publication_short: GoalsRL Workshop at FAIM
publication_types: ['1']
selected: false
title: Multi-task Maximum Causal Entropy Inverse Reinforcement Learning
url_code: https://github.com/HumanCompatibleAI/population-irl/
url_pdf: https://arxiv.org/pdf/1805.08882.pdf
url_dataset: ''
url_project: ''
url_slides: /slides/2018-multi-task-irl-poster.pdf
url_video: ''
---

